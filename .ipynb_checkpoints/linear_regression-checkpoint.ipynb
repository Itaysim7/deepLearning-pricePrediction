{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss:  49196438000.0 \n",
      "\n",
      "Iteration: 1000 Loss:  4169769700.0 \n",
      "\n",
      "Iteration: 2000 Loss:  4155272400.0 \n",
      "\n",
      "Iteration: 3000 Loss:  4141013800.0 \n",
      "\n",
      "Iteration: 4000 Loss:  4126988800.0 \n",
      "\n",
      "Iteration: 5000 Loss:  4113193500.0 \n",
      "\n",
      "Iteration: 6000 Loss:  4099622700.0 \n",
      "\n",
      "Iteration: 7000 Loss:  4086272800.0 \n",
      "\n",
      "Iteration: 8000 Loss:  4073141500.0 \n",
      "\n",
      "Iteration: 9000 Loss:  4060222500.0 \n",
      "\n",
      "Iteration: 10000 Loss:  4047514400.0 \n",
      "\n",
      "Iteration: 11000 Loss:  4035012900.0 \n",
      "\n",
      "Iteration: 12000 Loss:  4022714000.0 \n",
      "\n",
      "Iteration: 13000 Loss:  4010613800.0 \n",
      "\n",
      "Iteration: 14000 Loss:  3998711000.0 \n",
      "\n",
      "Iteration: 15000 Loss:  3986998300.0 \n",
      "\n",
      "Iteration: 16000 Loss:  3975477200.0 \n",
      "\n",
      "Iteration: 17000 Loss:  3964140000.0 \n",
      "\n",
      "Iteration: 18000 Loss:  3952987600.0 \n",
      "\n",
      "Iteration: 19000 Loss:  3942012700.0 \n",
      "\n",
      "Iteration: 20000 Loss:  3931215400.0 \n",
      "\n",
      "Iteration: 21000 Loss:  3920590800.0 \n",
      "\n",
      "Iteration: 22000 Loss:  3910135600.0 \n",
      "\n",
      "Iteration: 23000 Loss:  3899850000.0 \n",
      "\n",
      "Iteration: 24000 Loss:  3889727700.0 \n",
      "\n",
      "Iteration: 25000 Loss:  3879767800.0 \n",
      "\n",
      "Iteration: 26000 Loss:  3869965800.0 \n",
      "\n",
      "Iteration: 27000 Loss:  3860321500.0 \n",
      "\n",
      "Iteration: 28000 Loss:  3850829800.0 \n",
      "\n",
      "Iteration: 29000 Loss:  3841489200.0 \n",
      "\n",
      "Iteration: 30000 Loss:  3832296400.0 \n",
      "\n",
      "Iteration: 31000 Loss:  3823250700.0 \n",
      "\n",
      "Iteration: 32000 Loss:  3814346000.0 \n",
      "\n",
      "Iteration: 33000 Loss:  3805584000.0 \n",
      "\n",
      "Iteration: 34000 Loss:  3796960300.0 \n",
      "\n",
      "Iteration: 35000 Loss:  3788473000.0 \n",
      "\n",
      "Iteration: 36000 Loss:  3780119000.0 \n",
      "\n",
      "Iteration: 37000 Loss:  3771896000.0 \n",
      "\n",
      "Iteration: 38000 Loss:  3763803000.0 \n",
      "\n",
      "Iteration: 39000 Loss:  3755836000.0 \n",
      "\n",
      "Iteration: 40000 Loss:  3747994000.0 \n",
      "\n",
      "Iteration: 41000 Loss:  3740275000.0 \n",
      "\n",
      "Iteration: 42000 Loss:  3732676000.0 \n",
      "\n",
      "Iteration: 43000 Loss:  3725197000.0 \n",
      "\n",
      "Iteration: 44000 Loss:  3717833200.0 \n",
      "\n",
      "Iteration: 45000 Loss:  3710584600.0 \n",
      "\n",
      "Iteration: 46000 Loss:  3703449000.0 \n",
      "\n",
      "Iteration: 47000 Loss:  3696423000.0 \n",
      "\n",
      "Iteration: 48000 Loss:  3689505500.0 \n",
      "\n",
      "Iteration: 49000 Loss:  3682697700.0 \n",
      "\n",
      "Iteration: 50000 Loss:  3675991600.0 \n",
      "\n",
      "Iteration: 51000 Loss:  3669389800.0 \n",
      "\n",
      "Iteration: 52000 Loss:  3662891300.0 \n",
      "\n",
      "Iteration: 53000 Loss:  3656491500.0 \n",
      "\n",
      "Iteration: 54000 Loss:  3650190000.0 \n",
      "\n",
      "Iteration: 55000 Loss:  3643986200.0 \n",
      "\n",
      "Iteration: 56000 Loss:  3637876000.0 \n",
      "\n",
      "Iteration: 57000 Loss:  3631861000.0 \n",
      "\n",
      "Iteration: 58000 Loss:  3625936000.0 \n",
      "\n",
      "Iteration: 59000 Loss:  3620101400.0 \n",
      "\n",
      "Iteration: 60000 Loss:  3614357000.0 \n",
      "\n",
      "Iteration: 61000 Loss:  3608698400.0 \n",
      "\n",
      "Iteration: 62000 Loss:  3603124000.0 \n",
      "\n",
      "Iteration: 63000 Loss:  3597637000.0 \n",
      "\n",
      "Iteration: 64000 Loss:  3592231700.0 \n",
      "\n",
      "Iteration: 65000 Loss:  3586908700.0 \n",
      "\n",
      "Iteration: 66000 Loss:  3581664000.0 \n",
      "\n",
      "Iteration: 67000 Loss:  3576498200.0 \n",
      "\n",
      "Iteration: 68000 Loss:  3571412000.0 \n",
      "\n",
      "Iteration: 69000 Loss:  3566401000.0 \n",
      "\n",
      "Iteration: 70000 Loss:  3561464300.0 \n",
      "\n",
      "Iteration: 71000 Loss:  3556599800.0 \n",
      "\n",
      "Iteration: 72000 Loss:  3551809800.0 \n",
      "\n",
      "Iteration: 73000 Loss:  3547090700.0 \n",
      "\n",
      "Iteration: 74000 Loss:  3542444300.0 \n",
      "\n",
      "Iteration: 75000 Loss:  3537862700.0 \n",
      "\n",
      "Iteration: 76000 Loss:  3533348600.0 \n",
      "\n",
      "Iteration: 77000 Loss:  3528903700.0 \n",
      "\n",
      "Iteration: 78000 Loss:  3524522200.0 \n",
      "\n",
      "Iteration: 79000 Loss:  3520204000.0 \n",
      "\n",
      "Iteration: 80000 Loss:  3515953000.0 \n",
      "\n",
      "Iteration: 81000 Loss:  3511762200.0 \n",
      "\n",
      "Iteration: 82000 Loss:  3507631600.0 \n",
      "\n",
      "Iteration: 83000 Loss:  3503564500.0 \n",
      "\n",
      "Iteration: 84000 Loss:  3499552800.0 \n",
      "\n",
      "Iteration: 85000 Loss:  3495597600.0 \n",
      "\n",
      "Iteration: 86000 Loss:  3491704600.0 \n",
      "\n",
      "Iteration: 87000 Loss:  3487868400.0 \n",
      "\n",
      "Iteration: 88000 Loss:  3484083700.0 \n",
      "\n",
      "Iteration: 89000 Loss:  3480355300.0 \n",
      "\n",
      "Iteration: 90000 Loss:  3476685300.0 \n",
      "\n",
      "Iteration: 91000 Loss:  3473058000.0 \n",
      "\n",
      "Iteration: 92000 Loss:  3469492200.0 \n",
      "\n",
      "Iteration: 93000 Loss:  3465969000.0 \n",
      "\n",
      "Iteration: 94000 Loss:  3462502000.0 \n",
      "\n",
      "Iteration: 95000 Loss:  3459082200.0 \n",
      "\n",
      "Iteration: 96000 Loss:  3455712500.0 \n",
      "\n",
      "Iteration: 97000 Loss:  3452388000.0 \n",
      "\n",
      "Iteration: 98000 Loss:  3449112300.0 \n",
      "\n",
      "Iteration: 99000 Loss:  3445879000.0 \n",
      "\n",
      "Test error 3397205855.4066744\n",
      "test  3401161500.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Take all the data from the CSV file.and split it to price and mat of all fitchers\n",
    "data=pd.read_csv('Daegu_Real_Estate_data_28_feature.csv')\n",
    "data_y=data['SalePrice']\n",
    "data_x=data.drop('SalePrice',axis = 1)\n",
    "# data_x_scaled=(data_x-data_x.mean())/data_x.std()#normalization\n",
    "learning_rate = 0.0000001\n",
    "features =28\n",
    "\n",
    "#HallWayType change text values to numrical values.\n",
    "data_x['HallwayType'] =data_x['HallwayType'].astype('category')\n",
    "data_x['HallwayType'] =data_x['HallwayType'].cat.reorder_categories(['corridor','mixed','terraced'],ordered=True)\n",
    "data_x['HallwayType'] =data_x['HallwayType'].cat.codes\n",
    "\n",
    "#HeatingType change text values to numrical values.\n",
    "data_x['HeatingType'] =data_x['HeatingType'].astype('category')\n",
    "data_x['HeatingType'] =data_x['HeatingType'].cat.reorder_categories(['individual_heating','central_heating'],ordered=True)\n",
    "data_x['HeatingType'] =data_x['HeatingType'].cat.codes\n",
    "\n",
    "#AptManageType change text values to numrical values.\n",
    "data_x['AptManageType'] =data_x['AptManageType'].astype('category')\n",
    "data_x['AptManageType'] =data_x['AptManageType'].cat.reorder_categories(['self_management','management_in_trust'],ordered=True)\n",
    "data_x['AptManageType'] =data_x['AptManageType'].cat.codes\n",
    "\n",
    "#TimeToBusStop change text values to numrical values.\n",
    "data_x['TimeToSubway'] =data_x['TimeToSubway'].astype('category')\n",
    "data_x['TimeToSubway'] =data_x['TimeToSubway'].cat.reorder_categories(['no_bus_stop_nearby','15min~20min','10min~15min',\"5min~10min\",\"0-5min\"],ordered=True)\n",
    "data_x['TimeToSubway'] =data_x['TimeToSubway'].cat.codes\n",
    "\n",
    "#TimeToSubway change text values to numrical values.\n",
    "data_x['TimeToBusStop'] =data_x['TimeToBusStop'].astype('category')\n",
    "data_x['TimeToBusStop'] =data_x['TimeToBusStop'].cat.reorder_categories(['10min~15min','5min~10min','0~5min'],ordered=True)\n",
    "data_x['TimeToBusStop'] =data_x['TimeToBusStop'].cat.codes\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,test_size=0.3,random_state=0)\n",
    "\n",
    "new_x_train=np.array(x_train)\n",
    "new_y_train = np.array(y_train)\n",
    "new_x_test = np.array(x_test)\n",
    "new_y_test = np.array(y_test)\n",
    "\n",
    "new_y_train = new_y_train.reshape(y_train.shape[0],1)\n",
    "new_y_test = new_y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,features],name = 'x')\n",
    "y_ = tf.placeholder(tf.float32,[None,1],name = 'y_')\n",
    "W = tf.Variable(tf.random_normal([features,1]))\n",
    "B = tf.Variable(tf.zeros([1]))\n",
    "y = tf.matmul(x,W) + B\n",
    "loss = tf.reduce_mean(tf.pow(y-y_,2))\n",
    "update = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(0,100000):\n",
    "    sess.run(update,feed_dict={x:new_x_train , y_:new_y_train})\n",
    "    if i%1000==0:\n",
    "        print(\"Iteration:\",i,\"Loss: \", loss.eval(session=sess,feed_dict={x:new_x_train,y_:new_y_train}),\"\\n\")\n",
    "        \n",
    "print(\"Test error\", loss.eval(session=sess,feed_dict={x:new_x_test,y_:new_y_test}),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4123, 1)\n",
      "Iteration: 0  W1: [88.699715]  b: [0.04389329]  loss: 47447867000.0\n",
      "Iteration: 1000  W1: [10.088838]  b: [-0.01363548]  loss: 4260755500.0\n",
      "Iteration: 2000  W1: [16.396349]  b: [-0.02781812]  loss: 4243873800.0\n",
      "Iteration: 3000  W1: [22.613071]  b: [-0.0419499]  loss: 4227276500.0\n",
      "Iteration: 4000  W1: [28.743849]  b: [-0.05602973]  loss: 4210959600.0\n",
      "Iteration: 5000  W1: [34.789635]  b: [-0.070058]  loss: 4194917000.0\n",
      "Iteration: 6000  W1: [40.751255]  b: [-0.08403547]  loss: 4179144200.0\n",
      "Iteration: 7000  W1: [46.629528]  b: [-0.09796247]  loss: 4163636500.0\n",
      "Iteration: 8000  W1: [52.42528]  b: [-0.11183927]  loss: 4148388000.0\n",
      "Iteration: 9000  W1: [58.139385]  b: [-0.12566651]  loss: 4133397000.0\n",
      "[[2014 2017    8 ...    9   14   17]\n",
      " [2013 2013    5 ...    7    9   11]\n",
      " [2009 2013    7 ...    5    6    5]\n",
      " ...\n",
      " [2006 2011    1 ...    5    6    9]\n",
      " [2009 2009    7 ...    5    6    5]\n",
      " [1992 2010   11 ...    3    9   14]] \n",
      "\n",
      "session w: [[  63.766987 ]\n",
      " [ -55.73627  ]\n",
      " [  25.43969  ]\n",
      " [ 184.86655  ]\n",
      " [ 155.3735   ]\n",
      " [  30.672953 ]\n",
      " [   1.6771008]\n",
      " [   3.8355684]\n",
      " [ -63.073334 ]\n",
      " [  77.37699  ]\n",
      " [  -6.478293 ]\n",
      " [  41.7123   ]\n",
      " [ -17.751358 ]\n",
      " [ -26.420986 ]\n",
      " [ 110.337975 ]\n",
      " [ -80.77484  ]\n",
      " [ -23.438936 ]\n",
      " [  17.29546  ]\n",
      " [  -4.1386375]\n",
      " [ -91.38466  ]\n",
      " [   6.9892297]\n",
      " [ -40.18987  ]\n",
      " [ -43.4304   ]\n",
      " [ -65.10399  ]\n",
      " [ -71.886604 ]\n",
      " [  54.764095 ]\n",
      " [-178.35469  ]\n",
      " [-220.77174  ]]\n",
      "sesiion b: [-0.13943121]\n",
      "our Prediction: [[298783]\n",
      " [252858]\n",
      " [195849]\n",
      " ...\n",
      " [190006]\n",
      " [196224]\n",
      " [128000]]\n",
      "real values: 5867    470796\n",
      "2934    258079\n",
      "3068    211504\n",
      "4826    513274\n",
      "545     159292\n",
      "         ...  \n",
      "1884    170353\n",
      "3854    383716\n",
      "1686    201769\n",
      "772     143212\n",
      "1606     88495\n",
      "Name: SalePrice, Length: 1768, dtype: int64\n",
      "mean sqaure error 3881475938.8331447\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics\n",
    "\n",
    "tf.disable_v2_behavior() \n",
    "data = pd.read_csv('/home/orian/Desktop/DeepLearning/Daegu_Real_Estate_data.csv')\n",
    "data_y=data['SalePrice']\n",
    "data_x=data.drop(['SalePrice'],axis=1)\n",
    "\n",
    "#HallwayType\n",
    "data_x['HallwayType'] = data_x['HallwayType'].astype('category')\n",
    "data_x['HallwayType'] = data_x['HallwayType'].cat.reorder_categories(['corridor', 'mixed','terraced'], ordered=True)\n",
    "data_x['HallwayType'] = data_x['HallwayType'].cat.codes\n",
    "\n",
    "#HeatingType\n",
    "data_x['HeatingType'] = data_x['HeatingType'].astype('category')\n",
    "data_x['HeatingType'] = data_x['HeatingType'].cat.reorder_categories(['individual_heating', 'central_heating'], ordered=True)\n",
    "data_x['HeatingType'] = data_x['HeatingType'].cat.codes\n",
    "\n",
    "#\n",
    "data_x['AptManageType'] = data_x['AptManageType'].astype('category')#\n",
    "data_x['AptManageType'] = data_x['AptManageType'].cat.reorder_categories(['self_management', 'management_in_trust'], ordered=True)\n",
    "data_x['AptManageType'] = data_x['AptManageType'].cat.codes\n",
    "\n",
    "#TimeToBusStop\n",
    "data_x['TimeToBusStop'] = data_x['TimeToBusStop'].astype('category')\n",
    "data_x['TimeToBusStop'] = data_x['TimeToBusStop'].cat.reorder_categories(['10min~15min','5min~10min','0~5min'], ordered=True)\n",
    "data_x['TimeToBusStop'] = data_x['TimeToBusStop'].cat.codes\n",
    "\n",
    "#TimeToSubway\n",
    "data_x['TimeToSubway'] = data_x['TimeToSubway'].astype('category')\n",
    "data_x['TimeToSubway'] = data_x['TimeToSubway'].cat.reorder_categories(['no_bus_stop_nearby','15min~20min','10min~15min','5min~10min','0-5min'], ordered=True)\n",
    "data_x['TimeToSubway'] = data_x['TimeToSubway'].cat.codes\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data_x,data_y,test_size=0.30,random_state=1)\n",
    "new_y = np.array(y_train)\n",
    "new_x = np.array(x_train)\n",
    "features = 28\n",
    "learning_rate = 0.0000001\n",
    "training_epochs = 10000\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, features],name =\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None,1 ],name = \"y_\")\n",
    "W = tf.Variable(tf.random_uniform([features,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = tf.matmul(x,W) + b\n",
    "loss = tf.compat.v1.losses.mean_squared_error(\n",
    "    y_, y\n",
    ")\n",
    "update = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "sess = tf.Session()\n",
    "new_y = new_y.reshape(y_train.shape[0],1)\n",
    "\n",
    "print(new_y.shape)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "x1 = np.array(x_test)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,training_epochs):\n",
    "    sess.run(update, feed_dict = {x:x_train, y_:new_y})\n",
    "    if i%1000==0:\n",
    "        print('Iteration:' , i , ' W1:' , sess.run(W[0]) , ' b:' , sess.run(b), ' loss:', loss.eval(session=sess, feed_dict = {x:x_train, y_:new_y}))\n",
    "\n",
    "print(x1,\"\\n\")\n",
    "print(\"session w:\" ,sess.run(W))\n",
    "print(\"sesiion b:\" , sess.run(b))\n",
    "ourpred = tf.matmul(x_test,sess.run(W))+sess.run(b)\n",
    "print(\"our Prediction:\" , sess.run(ourpred))\n",
    "\n",
    "print(\"real values:\",y_test)\n",
    "\n",
    "print(\"mean sqaure error\",metrics.mean_squared_error(y_test,sess.run(ourpred)) )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  W: [[ 2.9280002]\n",
      " [17.960001 ]]  b: [0.514]  loss: 44111.715\n",
      "Iteration: 10000  W: [[-8.662441]\n",
      " [11.771298]]  b: [25.665333]  loss: 112.03969\n",
      "Iteration: 20000  W: [[-17.871014]\n",
      " [ 12.74393 ]]  b: [44.263157]  loss: 68.60526\n",
      "Iteration: 30000  W: [[-24.862799]\n",
      " [ 13.482421]]  b: [58.38387]  loss: 43.56598\n",
      "Iteration: 40000  W: [[-30.171246]\n",
      " [ 14.043115]]  b: [69.10478]  loss: 29.13128\n",
      "Iteration: 50000  W: [[-34.201736]\n",
      " [ 14.46882 ]]  b: [77.24501]  loss: 20.809607\n",
      "Iteration: 60000  W: [[-37.261272]\n",
      " [ 14.791976]]  b: [83.424126]  loss: 16.01305\n",
      "Iteration: 70000  W: [[-39.583096]\n",
      " [ 15.037201]]  b: [88.11371]  loss: 13.248581\n",
      "Iteration: 80000  W: [[-41.345997]\n",
      " [ 15.223399]]  b: [91.67423]  loss: 11.654467\n",
      "Iteration: 90000  W: [[-42.68493 ]\n",
      " [ 15.364842]]  b: [94.37757]  loss: 10.735038\n"
     ]
    }
   ],
   "source": [
    "features = 2\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "W = tf.Variable(tf.zeros([features,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = tf.matmul(x,W) + b\n",
    "loss = tf.reduce_mean(tf.pow(y - y_, 2))\n",
    "update = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "data_x = np.array([[2,4],[3,9],[4,16],[6,36],[7,49]])\n",
    "\n",
    "data_y = np.array([[70],[110],[165],[390],[550]])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(0,100000):\n",
    "    sess.run(update, feed_dict = {x:data_x, y_:data_y})\n",
    "    if i % 10000 == 0 :\n",
    "          print('Iteration:' , i , ' W:' , sess.run(W) , ' b:' , sess.run(b), ' loss:', loss.eval(session=sess, feed_dict = {x:data_x, y_:data_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
